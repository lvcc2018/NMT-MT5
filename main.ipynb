{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from transformers import AdamW, MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = 'google/mt5-small'\n",
    "model_path = './model/mt5_translation.pt'\n",
    "max_seq_len = 20\n",
    "device = torch.device('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MT5Tokenizer.from_pretrained(model_repo)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_repo)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('wmt17','zh-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_TOKEN_MAPPING = {\n",
    "    'en': '<en>',\n",
    "    'zh': '<zh>',\n",
    "    'de' : '<de>'\n",
    "}\n",
    "special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_str(text, target_lang, tokenizer, seq_len,\n",
    "                     lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  target_lang_token = lang_token_map[target_lang]\n",
    "\n",
    "  # Tokenize and add special tokens\n",
    "  input_ids = tokenizer.encode(\n",
    "      text = target_lang_token + text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "\n",
    "  return input_ids[0]\n",
    "  \n",
    "def encode_target_str(text, tokenizer, seq_len,\n",
    "                      lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  token_ids = tokenizer.encode(\n",
    "      text = text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "  \n",
    "  return token_ids[0]\n",
    "\n",
    "def format_translation_data(translations, lang_token_map,\n",
    "                            tokenizer, seq_len=128):\n",
    "  # Choose languages for in i/o\n",
    "  input_lang, target_lang = ['en', 'zh']\n",
    "\n",
    "  # Get the translations for the batch\n",
    "  input_text = translations[input_lang]\n",
    "  target_text = translations[target_lang]\n",
    "\n",
    "  if input_text is None or target_text is None:\n",
    "    return None\n",
    "\n",
    "  input_token_ids = encode_input_str(\n",
    "      input_text, target_lang, tokenizer, seq_len, lang_token_map)\n",
    "  \n",
    "  target_token_ids = encode_target_str(\n",
    "      target_text, tokenizer, seq_len, lang_token_map)\n",
    "\n",
    "  return input_token_ids, target_token_ids\n",
    "\n",
    "def transform_batch(batch, lang_token_map, tokenizer):\n",
    "  inputs = []\n",
    "  targets = []\n",
    "  for translation_set in batch['translation']:\n",
    "    formatted_data = format_translation_data(\n",
    "        translation_set, lang_token_map, tokenizer, max_seq_len)\n",
    "    \n",
    "    if formatted_data is None:\n",
    "      continue\n",
    "    \n",
    "    input_ids, target_ids = formatted_data\n",
    "    inputs.append(input_ids.unsqueeze(0))\n",
    "    targets.append(target_ids.unsqueeze(0))\n",
    "    \n",
    "  batch_input_ids = torch.cat(inputs).to(device)\n",
    "  batch_target_ids = torch.cat(targets).to(device)\n",
    "\n",
    "  return batch_input_ids, batch_target_ids\n",
    "\n",
    "def get_data_generator(dataset, lang_token_map, tokenizer, batch_size=32):\n",
    "  dataset = dataset.shuffle()\n",
    "  for i in range(0, len(dataset), batch_size):\n",
    "    raw_batch = dataset[i:i+batch_size]\n",
    "    yield transform_batch(raw_batch, lang_token_map, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing `data_transform`\n",
    "in_ids, out_ids = format_translation_data(\n",
    "    train_dataset[0]['translation'], LANG_TOKEN_MAPPING, tokenizer)\n",
    "\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(in_ids)))\n",
    "print(' '.join(tokenizer.convert_ids_to_tokens(out_ids)))\n",
    "\n",
    "# Testing data generator\n",
    "data_gen = get_data_generator(train_dataset, LANG_TOKEN_MAPPING, tokenizer, 8)\n",
    "data_batch = next(data_gen)\n",
    "print('Input shape:', data_batch[0].shape)\n",
    "print('Output shape:', data_batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "n_epochs = 8\n",
    "batch_size = 8\n",
    "print_freq = 50\n",
    "checkpoint_freq = 1000\n",
    "lr = 5e-4\n",
    "n_batches = int(np.ceil(len(train_dataset) / batch_size))\n",
    "total_steps = n_epochs * n_batches\n",
    "n_warmup_steps = int(total_steps * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, n_warmup_steps, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, gdataset, max_iters=8):\n",
    "  test_generator = get_data_generator(gdataset, LANG_TOKEN_MAPPING,\n",
    "                                      tokenizer, batch_size)\n",
    "  eval_losses = []\n",
    "  for i, (input_batch, label_batch) in enumerate(test_generator):\n",
    "    if i >= max_iters:\n",
    "      break\n",
    "\n",
    "    model_out = model.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "    eval_losses.append(model_out.loss.item())\n",
    "\n",
    "  return np.mean(eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_idx in range(n_epochs):\n",
    "  # Randomize data order\n",
    "  data_generator = get_data_generator(train_dataset, LANG_TOKEN_MAPPING,\n",
    "                                      tokenizer, batch_size)\n",
    "                \n",
    "  for batch_idx, (input_batch, label_batch) \\\n",
    "      in tqdm.notebook.tqdm(enumerate(data_generator), total=n_batches):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    model_out = model.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "\n",
    "    # Calculate loss and update weights\n",
    "    loss = model_out.loss\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print training update info\n",
    "    if (batch_idx + 1) % print_freq == 0:\n",
    "      avg_loss = np.mean(losses[-print_freq:])\n",
    "      print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {}'.format(\n",
    "          epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]))\n",
    "      \n",
    "    if (batch_idx + 1) % checkpoint_freq == 0:\n",
    "      test_loss = eval_model(model, test_dataset)\n",
    "      print('Saving model with test loss of {:.3f}'.format(test_loss))\n",
    "      torch.save(model.state_dict(), model_path)\n",
    "\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the loss\n",
    "window_size = 50\n",
    "smoothed_losses = []\n",
    "for i in range(len(losses)-window_size):\n",
    "  smoothed_losses.append(np.mean(losses[i:i+window_size]))\n",
    "\n",
    "plt.plot(smoothed_losses[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = test_dataset[0]['translation']['en']\n",
    "print('Raw input text:', test_sentence)\n",
    "\n",
    "input_ids = encode_input_str(\n",
    "    text = test_sentence,\n",
    "    target_lang = 'ja',\n",
    "    tokenizer = tokenizer,\n",
    "    seq_len = model.config.max_length,\n",
    "    lang_token_map = LANG_TOKEN_MAPPING)\n",
    "input_ids = input_ids.unsqueeze(0).to(device)\n",
    "\n",
    "print('Truncated input text:', tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens = model.generate(input_ids, num_beams=10, num_return_sequences=3)\n",
    "# print(output_tokens)\n",
    "for token_set in output_tokens:\n",
    "  print(tokenizer.decode(token_set, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a184b393f076f506e323259c37c3356f51af8ee1a6d9a6e29d35790321195045"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
